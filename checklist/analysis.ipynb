{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(df):\n",
    "    return accuracy_score(df['label'],df['preds'])\n",
    "\n",
    "def miss(df):\n",
    "    return df[df['label']!=df['preds']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['locationstest_babe_base.csv',\n",
       " 'factualtest_magpie.csv',\n",
       " 'prejudicetest_babe_base.csv',\n",
       " 'locationstest_magpie.csv',\n",
       " 'factualtest_roberta_anno_lexical.csv',\n",
       " 'factualtest_babe_base.csv',\n",
       " 'loadedtest_roberta_anno_lexical.csv',\n",
       " 'pronounstest_babe_base.csv',\n",
       " 'prejudicetest_magpie.csv',\n",
       " 'pronounstest_roberta_anno_lexical.csv',\n",
       " 'prejudicetest_roberta_anno_lexical.csv',\n",
       " 'locationstest_roberta_anno_lexical.csv',\n",
       " 'pronounstest_magpie.csv',\n",
       " 'loadedtest_magpie.csv',\n",
       " 'loadedtest_babe_base.csv']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data/out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFT: factual-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test we took two datasets of very simple, short, factual sentences and we want to test if our model can always predict them as unbiased. An example of a sentence from the dataset: \"Food cravings tell us that we want to eat a certain food.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "babe=pd.read_csv('data/out/factualtest_babe_base.csv')\n",
    "synth=pd.read_csv('data/out/factualtest_roberta_anno_lexical.csv')\n",
    "magpie=pd.read_csv('data/out/factualtest_magpie.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9506953223767383\n",
      "0.9149810366624526\n",
      "0.9639696586599241\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(babe))\n",
    "print(accuracy(magpie))\n",
    "print(accuracy(synth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What do we call a plant that grows where it is not wanted? Weed.',\n",
       " 'There is no place where it never stops snowing',\n",
       " 'Nothing in particular happens if you wear a hat made out of tin foil',\n",
       " 'Yes, duck quacks echo',\n",
       " 'People would disagree about the best beer in the world',\n",
       " 'J. B. Rhine attemped to prove the existence of ESP, but his methodology was flawed',\n",
       " 'Can animals dance like people? No, animals canâ€™t dance like people.',\n",
       " 'No, there are no carpets that can fly',\n",
       " \"British people don't have bad teeth\",\n",
       " 'What do we put on to dress up fancy? Fancy clothes and shoes.']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample of 10 sentences where babe classifier wrongly predicts biased but synthetic one doesnt\n",
    "miss(babe)[~miss(babe)['text'].isin(miss(synth)['text'])].text.sample(10).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIR: loaded-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test, we take unbiased sentences from the BABE test set and inject biased adjectives and adverbs into them, artifically changing the label to biased. The model succeeds if it can change it's prediction to biased. The biased adjectives and adverbs are taken from the bias lexicon. We only take neutral sentences that the model was able to classify as neutral in the first place, to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "babe=pd.read_csv('data/out/loadedtest_babe_base.csv')\n",
    "synth=pd.read_csv('data/out/loadedtest_roberta_anno_lexical.csv')\n",
    "magpie=pd.read_csv('data/out/loadedtest_magpie.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "babe = babe[babe['text_orig']!=babe['text_loaded']]\n",
    "magpie = magpie[magpie['text_orig']!=magpie['text_loaded']]\n",
    "synth = synth[synth['text_orig']!=synth['text_loaded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.412531328320802\n",
      "0.6838461538461539\n",
      "0.6644385026737968\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(babe))\n",
    "print(accuracy(magpie))\n",
    "print(accuracy(synth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INV: locations-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "babe=pd.read_csv('data/out/locationstest_babe_base.csv')\n",
    "synth=pd.read_csv('data/out/locationstest_roberta_anno_lexical.csv')\n",
    "magpie=pd.read_csv('data/out/locationstest_magpie.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9837004405286344\n",
      "0.9860986547085202\n",
      "0.9713968957871397\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(babe))\n",
    "print(accuracy(magpie))\n",
    "print(accuracy(synth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INV: pronouns-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "babe=pd.read_csv('data/out/pronounstest_babe_base.csv')\n",
    "synth=pd.read_csv('data/out/pronounstest_roberta_anno_lexical.csv')\n",
    "magpie=pd.read_csv('data/out/pronounstest_magpie.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9711884753901561\n",
      "0.9782608695652174\n",
      "0.9566787003610109\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(babe))\n",
    "print(accuracy(magpie))\n",
    "print(accuracy(synth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They has been teaching a course on terrorism and insurgency at it in Pittsburgh for four years, and much more of his class these days is devoted to white supremacy than in the past.'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss(magpie).text_ner_free.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Colin P. Clarke has been teaching a course on terrorism and insurgency at Carnegie Mellon University in Pittsburgh for four years, and much more of his class these days is devoted to white supremacy than in the past.'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss(magpie).text_orig.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INV: prejudice-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "babe=pd.read_csv('data/out/prejudicetest_babe_base.csv')\n",
    "synth=pd.read_csv('data/out/prejudicetest_roberta_anno_lexical.csv')\n",
    "magpie=pd.read_csv('data/out/prejudicetest_magpie.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "babe = babe[babe.label==0]\n",
    "synth = synth[synth.label==0]\n",
    "magpie = magpie[magpie.label==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8947368421052632\n",
      "0.891578947368421\n",
      "0.8526315789473684\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(babe))\n",
    "print(accuracy(magpie))\n",
    "print(accuracy(synth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politician\n",
      "babe 1.0\n",
      "magpie 1.0\n",
      "synth 1.0\n",
      "Gender\n",
      "babe 0.7133333333333334\n",
      "magpie 0.6866666666666666\n",
      "synth 0.5733333333333334\n",
      "Political Affiliation\n",
      "babe 0.6866666666666666\n",
      "magpie 0.76\n",
      "synth 0.8533333333333334\n",
      "Occupation\n",
      "babe 1.0\n",
      "magpie 0.9866666666666667\n",
      "synth 1.0\n",
      "Disability\n",
      "babe 1.0\n",
      "magpie 0.98\n",
      "synth 1.0\n",
      "Religion\n",
      "babe 0.9333333333333333\n",
      "magpie 0.8866666666666667\n",
      "synth 0.64\n",
      "Origin\n",
      "babe 1.0\n",
      "magpie 1.0\n",
      "synth 1.0\n"
     ]
    }
   ],
   "source": [
    "for category in babe.category.unique():\n",
    "    print(category)\n",
    "    print(f\"babe {accuracy(babe[babe.category==category])}\")\n",
    "    print(f\"magpie {accuracy(magpie[magpie.category==category])}\")\n",
    "    print(f\"synth {accuracy(synth[synth.category==category])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
